# Setup a kubernetes cluster
# idr-docker.yml must be run first
---

# Load hostvars (production OMERO)
- hosts: >
    {{ idr_environment | default('idr') }}-proxy-hosts


# Workaround this issue with Docker 1.13.1 and Kubernetes 1.7.4
# https://github.com/kubernetes/kubernetes/issues/40182#issuecomment-276392353
# We're going to drop these hosts so this is just a quick fix
- hosts: >
    {{ idr_environment | default('idr') }}-dockermanager-hosts
    {{ idr_environment | default('idr') }}-dockerworker-hosts

  tasks:

  - name: Workaround iptables problem with Docker 1.13.1 Kubernetes 1.7.4
    become: yes
    copy:
      src: files/docker-1.13-kube-1.7.4-iptables.service
      dest: /etc/systemd/system/docker-1.13-kube-1.7.4-iptables.service

  - name: Start and enable Docker Kubernetes Iptables workaround
    become: yes
    systemd:
      daemon_reload: yes
      name: docker-1.13-kube-1.7.4-iptables.service
      enabled: yes
      state: started


- hosts: >
    {{ idr_environment | default('idr') }}-dockermanager-hosts

  pre_tasks:

  - name: Get NFS IP
    set_fact:
      docker_nfs_host_ansible: >-
        {{
          hostvars[groups[
            idr_environment | default('idr') + '-dockermanager-hosts'][0]]
            ['ansible_' + (idr_net_iface | default('eth0'))]['ipv4']['address']
        }}

  - name: Get internal proxy IP for omero
    set_fact:
      proxy_omeroreadonly_endpoint_addresses: >-
        {{
          groups[idr_environment | default('idr') + '-proxy-hosts'] |
          map('extract', hostvars,
            ['ansible_' + (idr_net_iface | default('eth0')), 'ipv4', 'address']) | sort
        }}

  roles:
    - role: kubernetes
      kubernetes_role: master
      kubernetes_advertise_address: >-
        {{ hostvars[groups[idr_environment | default('idr') +
           '-dockermanager-hosts'][0]]['ansible_' +
           (idr_net_iface | default('eth0'))]['ipv4']['address']
        }}

  tasks:

  - name: Create kubernetes spec directory
    become: yes
    file:
      path: "{{ item }}"
      state: directory
    with_items:
    - /opt/kubernetes-spec
    - /opt/kubernetes-spec/nfs-volume-provisioner
    - /opt/kubernetes-spec/jupyterhub
    - /opt/kubernetes-spec/mineotaur

  - name: Copy kubernetes spec files
    become: yes
    copy:
      src: kubernetes-spec/{{ item }}
      dest: /opt/kubernetes-spec/{{ item }}
    with_items:
    - nfs-volume-provisioner/clusterrole.yaml
    - nfs-volume-provisioner/clusterrolebinding.yaml
    - nfs-volume-provisioner/serviceaccount.yaml
    - nfs-volume-provisioner/deployment.yaml
    - nfs-volume-provisioner/class.yaml

    #- jupyterhub/config.yml
    # Requires the hash of the config file (see later tasks)
    #- jupyterhub/deployment.yml
    - jupyterhub/nfsvolume.yml

    - mineotaur/deployment.yml

  - name: Jupyterhub kubernetes config file
    become: yes
    template:
      src: "{{ idr_jupyterhub_config_file }}"
      dest: /opt/kubernetes-spec/jupyterhub/config.yml

  # `kubectl apply` automaticallty restarts deployments if the spec
  # changes, but not if a ConfigMap changes.
  # We can force an update by including a hash of the config file in the
  # deployment spec: https://stackoverflow.com/q/37317003
  # Related issue: https://github.com/kubernetes/kubernetes/issues/22368
  - name: Hash kubernetes configuration files
    stat:
      path: /opt/kubernetes-spec/jupyterhub/config.yml
    register: _kubernetes_jupyterhub_config_st

  - name: Get kubernetes jupyterhub config hash
    set_fact:
      kubernetes_jupyterhub_config_hash: "{{ _kubernetes_jupyterhub_config_st.stat.checksum }}"

  - name: Template kubernetes spec templates
    become: yes
    template:
      src: kubernetes-spec/{{ item }}
      dest: /opt/kubernetes-spec/{{ item }}
    with_items:
    - mineotaur/nfsvolume.yml
    # Requires kubernetes_jupyterhub_config_hash
    - jupyterhub/deployment.yml


- hosts: >
    {{ idr_environment | default('idr') }}-dockerworker-hosts
  roles:
    - role: kubernetes
      kubernetes_role: worker
      kubernetes_token: >-
        {{ hostvars[groups[idr_environment | default('idr') +
           '-dockermanager-hosts'][0]].kubernetes_token
        }}
      kubernetes_master: >
        {{ hostvars[groups[idr_environment | default('idr') +
           '-dockermanager-hosts'][0]]['ansible_' +
           (idr_net_iface | default('eth0'))]['ipv4']['address']
        }}:6443


- hosts: >
    {{ idr_environment | default('idr') }}-dockermanager-hosts
    {{ idr_environment | default('idr') }}-dockerworker-hosts

  tasks:

  - name: jupyterhub | pull docker images in advance
    become: yes
    docker_image:
      name: "{{ item }}"
      state: "present"
      force: "{{ idr_docker_pull_latest }}"
    with_items: "{{ idr_prepull_docker_images }}"

  vars:
  - idr_docker_pull_latest: True


# Now run:
# kubectl get nodes
# kubectl apply -f config.yml -f jupyterhub-deployment.yml
# kubectl logs -f deployment/jupyterhub-deployment
