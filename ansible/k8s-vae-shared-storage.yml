# Shared dynamic storage for the VAE

# The gluster client is required for the two stage heketi storage setup
- hosts: "{{ idr_environment | default('idr') }}-k8svae-hosts"
  tasks:
  - name: Install gluster mount client
    become: yes
    yum:
      name: glusterfs-fuse
      state: present


- hosts: "{{ idr_environment | default('idr') }}-k8svae-master-hosts"
  become: yes

  pre_tasks:

  - name: Create LVM gluster reset script
    copy:
      content: |
        #!/usr/bin/bash
        export PATH=/usr/local/bin:$PATH
        cd ~/gluster-kubernetes/deploy
        ./gk-deploy -n gluster --abort -g -y
        kubectl delete storageclass gluster
        vgremove -f -y $(vgs --noheadings | awk '{print $1}')
        pvremove -f -y /dev/vdb
        rm -f ~/gk-deployed.flag
      dest: ~/lvm-reset.sh
      mode: 0755

  - name: Install LVM2 tools (useful for investigating problems)
    become: yes
    yum:
      name: lvm2
      state: present

  roles:
  - role: openmicroscopy.glusterfs-kube-config
    gluster_k8s_storage_group: "{{ idr_environment | default('idr') }}-k8svae-master-hosts"
    gluster_k8s_add_bin_path: /usr/local/bin
    gluster_kubernetes_version: d86e9237328f8dd95f50cd7be84ee6675fa908de
    gluster_kubernetes_repo: "https://github.com/manics/gluster-kubernetes.git"


# Setup some prerequisites for the VAE JupyterHub
# This takes advantage of the kubespray/library/kube.py module

- hosts: "{{ idr_environment | default('idr') }}-k8svae-master-hosts"
  run_once: true

  tasks:

  - name: Create kubernetes spec directory
    become: yes
    file:
      path: "{{ item }}"
      state: directory
    with_items:
    - /opt/kubernetes-spec
    - /opt/kubernetes-spec/jupyterhub-vae

  - name: Copy kubernetes manifests
    become: yes
    template:
      src: "{{ item.file }}"
      dest: "{{ item.dest }}"
    with_items:
    - file: kubernetes-spec/jupyterhub-vae/namespace.yaml
      type: Namespace
      dest: /opt/kubernetes-spec/jupyterhub-vae/namespace.yaml
    - file: kubernetes-spec/jupyterhub-vae/pvc-shared.yaml
      type: PersistentVolumeClaim
      dest: /opt/kubernetes-spec/jupyterhub-vae/pvc-shared.yaml
    register: kube_manifests

  - name: Apply kubernetes manifests
    become: yes
    kube:
      namespace: "{{ jupyterhub_vae_namespace }}"
      kubectl: "{{ kubectl_path }}"
      resource: "{{ item.item.type }}"
      filename: "{{ item.item.dest }}"
      state: "{{ item.changed | ternary('latest', 'present') }}"
    with_items: "{{ kube_manifests.results }}"

  vars:
    # This must match the namespace that the VAE will be deployed into
    jupyterhub_vae_namespace: vae-gh
    kubectl_path: /usr/local/bin/kubectl
